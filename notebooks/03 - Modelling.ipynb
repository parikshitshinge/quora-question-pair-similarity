{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pair Similarity\n",
    "### Kaggle Competition link: https://www.kaggle.com/c/quora-question-pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have built features to train the model on. Here we will load data with all our 627 features. We will first build a random or simple (Naive Bayes) base model and then will try out different machine learning algorithms and compare against our base model. After that, we will choose the best one and tune it to generalize it on future data.</p> \n",
    "<p> The metrics we will evaluate the models on are:<br>\n",
    "* log-loss <br>\n",
    "* Binary Confusion Matrix <br> \n",
    "</p>\n",
    "\n",
    "Our strategy is:\n",
    "1. Load the data\n",
    "2. Split data into train test (70:30)\n",
    "3. Normalize data\n",
    "4. <b>Build random model:</b> A model that randomly assigns probabilities.\n",
    "5. Apply models with default parameters:<br>\n",
    "   i. <b>Build Logistic Regression:</b> A statistical model that uses a logistic function to model the probability of a binary response based on one or more predictor variables.<br>\n",
    "   ii. <b>Build Naive Bayes:</b> A probabilistic algorithm based on Bayes' theorem that assumes the independence of the features in the input data<br>\n",
    "   iii. <b>Build Support Vector Machines:</b> Works by finding the best hyperplane that separates different classes of data points<br>\n",
    "   iv. <b>Build Gradient Boosting:</b> A powerful ensemble method that combines multiple weak models to create a strong classifier<br>\n",
    "   v: <b>XG Boost:</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# General\n",
    "from datetime import datetime \n",
    "import pickle\n",
    "\n",
    "# Data \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load data from SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Time taken: 0:02:47.199440\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "try:\n",
    "    conn = sqlite3.connect(\"train.db\")\n",
    "    data = pd.read_sql_query(\"SELECT * FROM train_data ORDER BY RANDOM() LIMIT 100000\", conn)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Data loaded!\\nTime taken: {0}\".format(datetime.now()-start))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (100000, 634)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: {0}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data after removing unnecessary columns: (100000, 628)\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "data = data.iloc[:,6:]\n",
    "print(\"Shape of data after removing unnecessary columns: {0}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_frequency</th>\n",
       "      <th>q2_frequency</th>\n",
       "      <th>q1_length</th>\n",
       "      <th>q2_length</th>\n",
       "      <th>q1_tokens_count</th>\n",
       "      <th>q2_tokens_count</th>\n",
       "      <th>q1_words_count</th>\n",
       "      <th>q2_words_count</th>\n",
       "      <th>q1_nonstopwords_count</th>\n",
       "      <th>...</th>\n",
       "      <th>q2_feat_291</th>\n",
       "      <th>q2_feat_292</th>\n",
       "      <th>q2_feat_293</th>\n",
       "      <th>q2_feat_294</th>\n",
       "      <th>q2_feat_295</th>\n",
       "      <th>q2_feat_296</th>\n",
       "      <th>q2_feat_297</th>\n",
       "      <th>q2_feat_298</th>\n",
       "      <th>q2_feat_299</th>\n",
       "      <th>q2_feat_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.370560</td>\n",
       "      <td>2.843680</td>\n",
       "      <td>3.062520</td>\n",
       "      <td>59.516500</td>\n",
       "      <td>60.036070</td>\n",
       "      <td>12.43623</td>\n",
       "      <td>12.679110</td>\n",
       "      <td>10.944490</td>\n",
       "      <td>11.172000</td>\n",
       "      <td>5.645940</td>\n",
       "      <td>...</td>\n",
       "      <td>46.830614</td>\n",
       "      <td>-27.859523</td>\n",
       "      <td>12.504153</td>\n",
       "      <td>-11.290065</td>\n",
       "      <td>-43.511224</td>\n",
       "      <td>-3.299467</td>\n",
       "      <td>26.084432</td>\n",
       "      <td>-20.810463</td>\n",
       "      <td>-66.571130</td>\n",
       "      <td>36.151218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482957</td>\n",
       "      <td>4.511923</td>\n",
       "      <td>6.084101</td>\n",
       "      <td>29.852672</td>\n",
       "      <td>33.841389</td>\n",
       "      <td>6.05970</td>\n",
       "      <td>7.071772</td>\n",
       "      <td>5.410749</td>\n",
       "      <td>6.300228</td>\n",
       "      <td>3.065338</td>\n",
       "      <td>...</td>\n",
       "      <td>60.902564</td>\n",
       "      <td>50.678152</td>\n",
       "      <td>69.079162</td>\n",
       "      <td>60.120607</td>\n",
       "      <td>56.651796</td>\n",
       "      <td>59.411909</td>\n",
       "      <td>57.067377</td>\n",
       "      <td>68.737010</td>\n",
       "      <td>67.284168</td>\n",
       "      <td>58.686727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1181.601844</td>\n",
       "      <td>-869.310289</td>\n",
       "      <td>-1082.525471</td>\n",
       "      <td>-1091.648775</td>\n",
       "      <td>-773.917976</td>\n",
       "      <td>-768.730118</td>\n",
       "      <td>-402.046199</td>\n",
       "      <td>-864.229748</td>\n",
       "      <td>-2196.106435</td>\n",
       "      <td>-364.145416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.555332</td>\n",
       "      <td>-54.939070</td>\n",
       "      <td>-21.404714</td>\n",
       "      <td>-42.356623</td>\n",
       "      <td>-69.761781</td>\n",
       "      <td>-36.626309</td>\n",
       "      <td>-6.141048</td>\n",
       "      <td>-57.998502</td>\n",
       "      <td>-96.638794</td>\n",
       "      <td>0.665372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>41.185087</td>\n",
       "      <td>-26.160044</td>\n",
       "      <td>14.176917</td>\n",
       "      <td>-10.020232</td>\n",
       "      <td>-35.516233</td>\n",
       "      <td>-5.113182</td>\n",
       "      <td>22.142940</td>\n",
       "      <td>-18.186085</td>\n",
       "      <td>-57.047950</td>\n",
       "      <td>28.246542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>77.560943</td>\n",
       "      <td>-0.450381</td>\n",
       "      <td>49.923865</td>\n",
       "      <td>21.925329</td>\n",
       "      <td>-8.349561</td>\n",
       "      <td>27.594170</td>\n",
       "      <td>54.677847</td>\n",
       "      <td>18.421707</td>\n",
       "      <td>-24.613595</td>\n",
       "      <td>62.833737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>1169.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1487.891279</td>\n",
       "      <td>774.577598</td>\n",
       "      <td>779.584843</td>\n",
       "      <td>852.355989</td>\n",
       "      <td>569.228259</td>\n",
       "      <td>907.389180</td>\n",
       "      <td>661.972040</td>\n",
       "      <td>574.299960</td>\n",
       "      <td>371.700851</td>\n",
       "      <td>1080.074251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate   q1_frequency   q2_frequency      q1_length   \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \\\n",
       "mean        0.370560       2.843680       3.062520      59.516500   \n",
       "std         0.482957       4.511923       6.084101      29.852672   \n",
       "min         0.000000       1.000000       1.000000       1.000000   \n",
       "25%         0.000000       1.000000       1.000000      39.000000   \n",
       "50%         0.000000       1.000000       1.000000      52.000000   \n",
       "75%         1.000000       3.000000       2.000000      72.000000   \n",
       "max         1.000000      50.000000     120.000000     354.000000   \n",
       "\n",
       "           q2_length  q1_tokens_count  q2_tokens_count  q1_words_count   \n",
       "count  100000.000000     100000.00000    100000.000000   100000.000000  \\\n",
       "mean       60.036070         12.43623        12.679110       10.944490   \n",
       "std        33.841389          6.05970         7.071772        5.410749   \n",
       "min         1.000000          1.00000         1.000000        1.000000   \n",
       "25%        39.000000          9.00000         8.000000        7.000000   \n",
       "50%        51.000000         11.00000        11.000000       10.000000   \n",
       "75%        72.000000         15.00000        15.000000       13.000000   \n",
       "max      1169.000000        100.00000       272.000000       71.000000   \n",
       "\n",
       "       q2_words_count  q1_nonstopwords_count  ...    q2_feat_291   \n",
       "count   100000.000000          100000.000000  ...  100000.000000  \\\n",
       "mean        11.172000               5.645940  ...      46.830614   \n",
       "std          6.300228               3.065338  ...      60.902564   \n",
       "min          1.000000               0.000000  ...   -1181.601844   \n",
       "25%          7.000000               4.000000  ...      10.555332   \n",
       "50%         10.000000               5.000000  ...      41.185087   \n",
       "75%         13.000000               7.000000  ...      77.560943   \n",
       "max        237.000000              37.000000  ...    1487.891279   \n",
       "\n",
       "         q2_feat_292    q2_feat_293    q2_feat_294    q2_feat_295   \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \\\n",
       "mean      -27.859523      12.504153     -11.290065     -43.511224   \n",
       "std        50.678152      69.079162      60.120607      56.651796   \n",
       "min      -869.310289   -1082.525471   -1091.648775    -773.917976   \n",
       "25%       -54.939070     -21.404714     -42.356623     -69.761781   \n",
       "50%       -26.160044      14.176917     -10.020232     -35.516233   \n",
       "75%        -0.450381      49.923865      21.925329      -8.349561   \n",
       "max       774.577598     779.584843     852.355989     569.228259   \n",
       "\n",
       "         q2_feat_296    q2_feat_297    q2_feat_298    q2_feat_299   \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \\\n",
       "mean       -3.299467      26.084432     -20.810463     -66.571130   \n",
       "std        59.411909      57.067377      68.737010      67.284168   \n",
       "min      -768.730118    -402.046199    -864.229748   -2196.106435   \n",
       "25%       -36.626309      -6.141048     -57.998502     -96.638794   \n",
       "50%        -5.113182      22.142940     -18.186085     -57.047950   \n",
       "75%        27.594170      54.677847      18.421707     -24.613595   \n",
       "max       907.389180     661.972040     574.299960     371.700851   \n",
       "\n",
       "         q2_feat_300  \n",
       "count  100000.000000  \n",
       "mean       36.151218  \n",
       "std        58.686727  \n",
       "min      -364.145416  \n",
       "25%         0.665372  \n",
       "50%        28.246542  \n",
       "75%        62.833737  \n",
       "max      1080.074251  \n",
       "\n",
       "[8 rows x 628 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split data into train test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100000, 627)\n",
      "Shape of y: (100000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into X & y first\n",
    "X = data.drop('is_duplicate', axis=1)\n",
    "y = data['is_duplicate']\n",
    "\n",
    "print(\"Shape of X: {0}\".format(X.shape))\n",
    "print(\"Shape of y: {0}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (70000, 627)\n",
      "Shape of X_test: (30000, 627)\n",
      "Shape of y_train: (70000,)\n",
      "Shape of y_test: (30000,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train & test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y)\n",
    "print(\"Shape of X_train: {0}\".format(X_train.shape))\n",
    "print(\"Shape of X_test: {0}\".format(X_test.shape))\n",
    "print(\"Shape of y_train: {0}\".format(y_train.shape))\n",
    "print(\"Shape of y_test: {0}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of target variable in train\n",
      "Class 0: 62.94428571428572 % \n",
      "Class 1: 37.05571428571429 %\n",
      "\n",
      "Distribution of target variable in test\n",
      "Class 0: 62.94333333333333 % \n",
      "Class 1: 37.056666666666665 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of target variable in train\")\n",
    "train_counter = Counter(y_train)\n",
    "train_len = len(y_train)\n",
    "print(\"Class 0: {0} % \\nClass 1: {1} %\".format((train_counter[0]/train_len)*100, (train_counter[1]/train_len)*100))\n",
    "\n",
    "\n",
    "print(\"\\nDistribution of target variable in test\")\n",
    "test_counter = Counter(y_test)\n",
    "test_len = len(y_test)\n",
    "print(\"Class 0: {0} % \\nClass 1: {1} %\".format((test_counter[0]/test_len)*100, (test_counter[1]/test_len)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 0\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normalize data\n",
    "Before we proceed to build the models, lets normalize all the features first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X_train\n",
      "Normalization of X_train is completed.\n",
      "\n",
      "Time taken: 0:00:05.002509\n",
      "\n",
      "Normalizing X_test\n",
      "Normalization of X_test is completed.\n",
      "\n",
      "Time taken: 0:00:00.436034\n"
     ]
    }
   ],
   "source": [
    "numerical_pipeline = Pipeline(steps=[(\"normalizer\", MinMaxScaler()), ((\"imputer\", SimpleImputer(strategy=\"most_frequent\")))])\n",
    "\n",
    "vectorizer = ColumnTransformer([(\"num_pipeline\", numerical_pipeline, numerical_features)])\n",
    "\n",
    "start = datetime.now()\n",
    "print(\"Vectorizing X_train\")\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "print(\"Normalization of X_train is completed.\\n\\nTime taken: {0}\".format(datetime.now()-start))\n",
    "\n",
    "start = datetime.now()\n",
    "print(\"\\nNormalizing X_test\")\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print(\"Normalization of X_test is completed.\\n\\nTime taken: {0}\".format(datetime.now()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped the vectorizer in ../models/vectorizer.pkl file\n"
     ]
    }
   ],
   "source": [
    "# Lets save our vectorizer to .pkl file\n",
    "vectorizer_file = \"../models/vectorizer.pkl\"\n",
    "with open(vectorizer_file, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print('Dumped the vectorizer in {} file'.format(vectorizer_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build random model\n",
    "Here we will randomly assign a class based on random probability to each test data point and measure its log loss.<br>\n",
    "A strategy we will follow for this is:\n",
    "1. Generatea list of 2 random numbers for each test row\n",
    "2. Divide each random number by its sum so we get their sum as 1\n",
    "3. Take the index of maximum of the 2 numbers in the list\n",
    "4. This index will be the class of given test row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test log-loss of random model: 0.89532403134099\n",
      "\n",
      "Test accuracy score of random model: 0.4961\n",
      "\n",
      "Test confusion matrix of random model: \n",
      "[[9373 9554]\n",
      " [5563 5510]]\n",
      "\n",
      "Test confusion matrix of random model (%): \n",
      "[[31.24 31.85]\n",
      " [18.54 18.37]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = np.zeros((test_len,2))\n",
    "for i in range(test_len):\n",
    "    random_probs = np.random.rand(1,2)\n",
    "    y_pred_prob[i] = ((random_probs/sum(sum(random_probs)))[0])\n",
    "\n",
    "print(\"Test log-loss of random model: {0}\".format(log_loss(y_test, y_pred_prob, eps=1e-15)))\n",
    "\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "print(\"\\nTest accuracy score of random model: {0}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print(\"\\nTest confusion matrix of random model: \\n{0}\".format(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "print(\"\\nTest confusion matrix of random model (%): \\n{0}\".format(np.round(confusion_matrix(y_test, y_pred)/len(y_test)*100,2)))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil ltake this as benchmark to compare our future models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Apply ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression started.\n",
      "LogisticRegression training completed. Time taken: 0:00:47.715623\n",
      "\n",
      "BernoulliNB started.\n",
      "BernoulliNB training completed. Time taken: 0:00:00.554632\n",
      "\n",
      "SVC started.\n",
      "SVC training completed. Time taken: 0:22:43.016640\n",
      "\n",
      "GradientBoostingClassifier started.\n",
      "GradientBoostingClassifier training completed. Time taken: 0:51:01.605331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for classifier in [LogisticRegression(solver='lbfgs', max_iter=3000), BernoulliNB(), SVC(), GradientBoostingClassifier() ]:\n",
    "    \n",
    "    # Training\n",
    "    start = datetime.now()\n",
    "    clf_str = str(classifier).split(\"(\")[0]\n",
    "    print(\"{0} started.\".format(clf_str))\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"{0} training completed. Time taken: {1}\\n\".format(clf_str, datetime.now()-start))\n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    lg_loss = log_loss(y_test,y_pred)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    cm = np.round((confusion_matrix(y_test,y_pred)/len(X_test)*100),2)\n",
    "    \n",
    "    # Add to result\n",
    "    temp = list()\n",
    "    temp.append(clf_str)\n",
    "    temp.append(\"Default\")\n",
    "    temp.append(lg_loss)\n",
    "    temp.append(acc)\n",
    "    temp.append(cm)\n",
    "    temp.append(datetime.now()-start)\n",
    "    result.append(temp)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train XGBoost using sklearn API (not Learning API). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost started.\n",
      "XGBoost training completed. Time taken: 0:06:35.676606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "start = datetime.now()\n",
    "xgbst = XGBClassifier()\n",
    "\n",
    "print(\"XGBoost started.\")\n",
    "xgbst.fit(X_train, y_train)\n",
    "print(\"XGBoost training completed. Time taken: {0}\\n\".format(datetime.now()-start))\n",
    "\n",
    "# Prediction\n",
    "y_pred = xgbst.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "lg_loss = log_loss(y_test,y_pred)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "cm = np.round((confusion_matrix(y_test,y_pred)/len(X_test)*100),2)\n",
    "\n",
    "# Add to result\n",
    "temp = list()\n",
    "temp.append(\"XGBoost\")\n",
    "temp.append(\"Default\")\n",
    "temp.append(lg_loss)\n",
    "temp.append(acc)\n",
    "temp.append(cm)\n",
    "temp.append(datetime.now()-start)\n",
    "result.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Log-loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Confusion Matrix (TP,FP,FN,TN)</th>\n",
       "      <th>Time taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>Default</td>\n",
       "      <td>8.010101</td>\n",
       "      <td>0.777767</td>\n",
       "      <td>[[53.57, 9.52], [12.7, 24.21]]</td>\n",
       "      <td>0 days 00:00:47.783151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>Default</td>\n",
       "      <td>12.445874</td>\n",
       "      <td>0.654700</td>\n",
       "      <td>[[38.9, 24.19], [10.34, 26.57]]</td>\n",
       "      <td>0 days 00:00:00.805230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>Default</td>\n",
       "      <td>7.282019</td>\n",
       "      <td>0.797967</td>\n",
       "      <td>[[53.89, 9.2], [11.0, 25.91]]</td>\n",
       "      <td>0 days 00:34:24.827754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>Default</td>\n",
       "      <td>6.320855</td>\n",
       "      <td>0.824633</td>\n",
       "      <td>[[56.07, 7.02], [10.51, 26.4]]</td>\n",
       "      <td>0 days 00:51:01.899364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Default</td>\n",
       "      <td>5.989254</td>\n",
       "      <td>0.833833</td>\n",
       "      <td>[[55.71, 7.38], [9.23, 27.68]]</td>\n",
       "      <td>0 days 00:06:35.918235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Algorithm Hyperparameters   Log-loss  Accuracy   \n",
       "0          LogisticRegression         Default   8.010101  0.777767  \\\n",
       "1                 BernoulliNB         Default  12.445874  0.654700   \n",
       "2                         SVC         Default   7.282019  0.797967   \n",
       "3  GradientBoostingClassifier         Default   6.320855  0.824633   \n",
       "4                     XGBoost         Default   5.989254  0.833833   \n",
       "\n",
       "    Confusion Matrix (TP,FP,FN,TN)             Time taken  \n",
       "0   [[53.57, 9.52], [12.7, 24.21]] 0 days 00:00:47.783151  \n",
       "1  [[38.9, 24.19], [10.34, 26.57]] 0 days 00:00:00.805230  \n",
       "2    [[53.89, 9.2], [11.0, 25.91]] 0 days 00:34:24.827754  \n",
       "3   [[56.07, 7.02], [10.51, 26.4]] 0 days 00:51:01.899364  \n",
       "4   [[55.71, 7.38], [9.23, 27.68]] 0 days 00:06:35.918235  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result, columns=['Algorithm', 'Hyperparameters', 'Log-loss', 'Accuracy', 'Confusion Matrix (TP,FP,FN,TN)', 'Time taken'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per above, XGBoostClassifier has given us the best result with minimal training time. We will hypertune this to see how we can improve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>common_nonstopwords_share</td>\n",
       "      <td>0.075802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1_frequency</td>\n",
       "      <td>0.022391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2_frequency</td>\n",
       "      <td>0.020055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>common_words_count_max</td>\n",
       "      <td>0.013781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fuzz_token_sort_ratio</td>\n",
       "      <td>0.012756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>common_tokens_count_min</td>\n",
       "      <td>0.007614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>common_nonstopwords_count</td>\n",
       "      <td>0.007455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>common_tokens_count</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>common_nonstopwords_count_min</td>\n",
       "      <td>0.007142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fuzz_token_set_ratio</td>\n",
       "      <td>0.007078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Feature  Importance\n",
       "15      common_nonstopwords_share    0.075802\n",
       "0                    q1_frequency    0.022391\n",
       "1                    q2_frequency    0.020055\n",
       "24         common_words_count_max    0.013781\n",
       "18          fuzz_token_sort_ratio    0.012756\n",
       "21        common_tokens_count_min    0.007614\n",
       "14      common_nonstopwords_count    0.007455\n",
       "10            common_tokens_count    0.007186\n",
       "25  common_nonstopwords_count_min    0.007142\n",
       "19           fuzz_token_set_ratio    0.007078"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(numerical_features, xgbst.feature_importances_), columns=['Feature', 'Importance']).sort_values(by='Importance', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "print(xgbst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV started.\n",
      "GridSearchCV completed. Time taken: 1 day, 2:30:28.404633\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"GridSearchCV started.\")\n",
    "#     'gamma' : [i/10.0 for i in range(0,4)],\n",
    "parameters = {\n",
    "    'n_estimators': (100,200),\n",
    "    'max_depth': [i for i in range(3,9,2)],\n",
    "    'reg_alpha' : [1e-2, 0.1, 1],\n",
    "    'learning_rate': [1e-2, 0.1, 0.25]\n",
    "}\n",
    "\n",
    "bst = XGBClassifier()\n",
    "clf = GridSearchCV(bst, parameters, return_train_score=True, scoring=\"neg_log_loss\", cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"GridSearchCV completed. Time taken: {0}\".format(datetime.now()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = pd.DataFrame.from_dict(clf.cv_results_)\n",
    "cv_result.to_csv(\"../models/cv_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>625.874169</td>\n",
       "      <td>15.887039</td>\n",
       "      <td>0.094382</td>\n",
       "      <td>0.014431</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>-0.326015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329985</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.148817</td>\n",
       "      <td>-0.145842</td>\n",
       "      <td>-0.148142</td>\n",
       "      <td>-0.148319</td>\n",
       "      <td>-0.152427</td>\n",
       "      <td>-0.148709</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>627.068382</td>\n",
       "      <td>34.131511</td>\n",
       "      <td>0.099374</td>\n",
       "      <td>0.019783</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>-0.328543</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330771</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.152286</td>\n",
       "      <td>-0.149122</td>\n",
       "      <td>-0.147583</td>\n",
       "      <td>-0.149894</td>\n",
       "      <td>-0.150018</td>\n",
       "      <td>-0.149780</td>\n",
       "      <td>0.001524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>657.869789</td>\n",
       "      <td>20.192025</td>\n",
       "      <td>0.096362</td>\n",
       "      <td>0.009753</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 7, 'n_esti...</td>\n",
       "      <td>-0.329161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331602</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.147848</td>\n",
       "      <td>-0.144877</td>\n",
       "      <td>-0.149461</td>\n",
       "      <td>-0.151994</td>\n",
       "      <td>-0.149598</td>\n",
       "      <td>-0.148755</td>\n",
       "      <td>0.002348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time   \n",
       "35     625.874169     15.887039         0.094382        0.014431  \\\n",
       "33     627.068382     34.131511         0.099374        0.019783   \n",
       "34     657.869789     20.192025         0.096362        0.009753   \n",
       "\n",
       "   param_learning_rate param_max_depth param_n_estimators param_reg_alpha   \n",
       "35                 0.1               7                200               1  \\\n",
       "33                 0.1               7                200            0.01   \n",
       "34                 0.1               7                200             0.1   \n",
       "\n",
       "                                               params  split0_test_score  ...   \n",
       "35  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...          -0.326015  ...  \\\n",
       "33  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...          -0.328543  ...   \n",
       "34  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...          -0.329161  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score   \n",
       "35        -0.329985        0.006394                1           -0.148817  \\\n",
       "33        -0.330771        0.004665                2           -0.152286   \n",
       "34        -0.331602        0.005049                3           -0.147848   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score   \n",
       "35           -0.145842           -0.148142           -0.148319  \\\n",
       "33           -0.149122           -0.147583           -0.149894   \n",
       "34           -0.144877           -0.149461           -0.151994   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "35           -0.152427         -0.148709         0.002123  \n",
       "33           -0.150018         -0.149780         0.001524  \n",
       "34           -0.149598         -0.148755         0.002348  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result.sort_values(by='mean_test_score', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped: GridSearchCV(cv=5,\n",
      "             estimator=XGBClassifier(base_score=None, booster=None,\n",
      "                                     callbacks=None, colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     early_stopping_rounds=None,\n",
      "                                     enable_categorical=False, eval_metric=None,\n",
      "                                     feature_types=None, gamma=None,\n",
      "                                     gpu_id=None, grow_policy=None,\n",
      "                                     importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None,...\n",
      "                                     max_delta_step=None, max_depth=None,\n",
      "                                     max_leaves=None, min_child_weight=None,\n",
      "                                     missing=nan, monotone_constraints=None,\n",
      "                                     n_estimators=100, n_jobs=None,\n",
      "                                     num_parallel_tree=None, predictor=None,\n",
      "                                     random_state=None, ...),\n",
      "             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n",
      "                         'max_depth': [3, 5, 7], 'n_estimators': (100, 200),\n",
      "                         'reg_alpha': [0.01, 0.1, 1]},\n",
      "             return_train_score=True, scoring='neg_log_loss')\n"
     ]
    }
   ],
   "source": [
    "file = open('gridsearchcv.pkl','wb')\n",
    "pickle.dump(clf, file)\n",
    "print('Dumped: {}'.format(clf))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost started with parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 1}\n",
      "XGBoost training completed. Time taken: 0:14:57.686727\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m temp\u001b[39m.\u001b[39mappend(cm)\n\u001b[0;32m     24\u001b[0m temp\u001b[39m.\u001b[39mappend(datetime\u001b[39m.\u001b[39mnow()\u001b[39m-\u001b[39mstart)\n\u001b[1;32m---> 25\u001b[0m result\u001b[39m.\u001b[39mappend(temp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "start = datetime.now()\n",
    "xgbst = XGBClassifier(learning_rate= 0.1, max_depth= 7, n_estimators= 200, reg_alpha= 1)\n",
    "\n",
    "print(\"XGBoost started with parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 1}\")\n",
    "xgbst.fit(X_train, y_train)\n",
    "print(\"XGBoost training completed. Time taken: {0}\\n\".format(datetime.now()-start))\n",
    "\n",
    "# Prediction\n",
    "y_pred = xgbst.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "lg_loss = log_loss(y_test,y_pred)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "cm = np.round((confusion_matrix(y_test,y_pred)/len(X_test)*100),2)\n",
    "\n",
    "# Add to result\n",
    "temp = list()\n",
    "temp.append(\"XGBoost\")\n",
    "temp.append(\"'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 1\")\n",
    "temp.append(lg_loss)\n",
    "temp.append(acc)\n",
    "temp.append(cm)\n",
    "temp.append(datetime.now()-start)\n",
    "result.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XGBoost',\n",
       " \"'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'reg_alpha': 1\",\n",
       " 5.871511137087184,\n",
       " 0.8371,\n",
       " array([[56.21,  6.73],\n",
       "        [ 9.56, 27.5 ]]),\n",
       " datetime.timedelta(seconds=897, microseconds=960767)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we brought log_loss down to 5.8715111 from 5.989254 after hyperparameters tuning. Lets save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "file = open('xgboost_tuned.pkl','wb')\n",
    "pickle.dump(xgbst, file)\n",
    "print('Dumped: {}'.format(xgbst))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
