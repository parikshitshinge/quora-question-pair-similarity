{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pair Similarity\n",
    "### Kaggle Competition link: https://www.kaggle.com/c/quora-question-pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have built features to train the model on. Here we will load data with all our 627 features. We will first build a random or simple (Naive Bayes) base model and then will try out different machine learning algorithms and compare against our base model. After that, we will choose the best one and tune it to generalize it on future data.</p> \n",
    "<p> The metrics we will evaluate the models on are:<br>\n",
    "* log-loss <br>\n",
    "* Binary Confusion Matrix <br> \n",
    "</p>\n",
    "\n",
    "Our strategy is:\n",
    "1. Load the data\n",
    "2. Split data into train test (70:30)\n",
    "3. <b>Build random model:</b> A model that randomly assigns probabilities.\n",
    "4. <b>Build Logistic Regression:</b> A statistical model that uses a logistic function to model the probability of a binary response based on one or more predictor variables.\n",
    "5. <b>Build Naive Bayes:</b> A probabilistic algorithm based on Bayes' theorem that assumes the independence of the features in the input data\n",
    "6. <b>Build Support Vector Machines:</b> Works by finding the best hyperplane that separates different classes of data points\n",
    "7. <b>Build Gradient Boosting:</b> A powerful ensemble method that combines multiple weak models to create a strong classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# General\n",
    "from datetime import datetime \n",
    "\n",
    "# Data \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load data from SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Time taken: 0:04:43.249457\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "try:\n",
    "    conn = sqlite3.connect(\"train.db\")\n",
    "    data = pd.read_sql_query(\"SELECT * FROM train_data\", conn)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Data loaded!\\nTime taken: {0}\".format(datetime.now()-start))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (404290, 634)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: {0}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data after removing unnecessary columns: (404290, 628)\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "data = data.iloc[:,6:]\n",
    "print(\"Shape of data after removing unnecessary columns: {0}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_frequency</th>\n",
       "      <th>q2_frequency</th>\n",
       "      <th>q1_length</th>\n",
       "      <th>q2_length</th>\n",
       "      <th>q1_tokens_count</th>\n",
       "      <th>q2_tokens_count</th>\n",
       "      <th>q1_words_count</th>\n",
       "      <th>q2_words_count</th>\n",
       "      <th>q1_nonstopwords_count</th>\n",
       "      <th>...</th>\n",
       "      <th>q2_feat_291</th>\n",
       "      <th>q2_feat_292</th>\n",
       "      <th>q2_feat_293</th>\n",
       "      <th>q2_feat_294</th>\n",
       "      <th>q2_feat_295</th>\n",
       "      <th>q2_feat_296</th>\n",
       "      <th>q2_feat_297</th>\n",
       "      <th>q2_feat_298</th>\n",
       "      <th>q2_feat_299</th>\n",
       "      <th>q2_feat_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404289.000000</td>\n",
       "      <td>404288.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "      <td>404290.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.369198</td>\n",
       "      <td>2.827609</td>\n",
       "      <td>3.046961</td>\n",
       "      <td>59.536856</td>\n",
       "      <td>60.108663</td>\n",
       "      <td>12.438626</td>\n",
       "      <td>12.697482</td>\n",
       "      <td>10.944592</td>\n",
       "      <td>11.185120</td>\n",
       "      <td>5.646781</td>\n",
       "      <td>...</td>\n",
       "      <td>46.763162</td>\n",
       "      <td>-28.009493</td>\n",
       "      <td>12.527336</td>\n",
       "      <td>-11.368422</td>\n",
       "      <td>-43.681345</td>\n",
       "      <td>-3.424754</td>\n",
       "      <td>26.239380</td>\n",
       "      <td>-20.802047</td>\n",
       "      <td>-66.431884</td>\n",
       "      <td>36.273753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482588</td>\n",
       "      <td>4.487418</td>\n",
       "      <td>6.026871</td>\n",
       "      <td>29.940546</td>\n",
       "      <td>33.863690</td>\n",
       "      <td>6.085369</td>\n",
       "      <td>7.080560</td>\n",
       "      <td>5.431949</td>\n",
       "      <td>6.311076</td>\n",
       "      <td>3.074383</td>\n",
       "      <td>...</td>\n",
       "      <td>61.070267</td>\n",
       "      <td>50.926218</td>\n",
       "      <td>68.963110</td>\n",
       "      <td>60.353412</td>\n",
       "      <td>56.719721</td>\n",
       "      <td>59.543628</td>\n",
       "      <td>57.218495</td>\n",
       "      <td>68.938349</td>\n",
       "      <td>66.916404</td>\n",
       "      <td>58.818664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1181.601844</td>\n",
       "      <td>-869.310289</td>\n",
       "      <td>-1082.525471</td>\n",
       "      <td>-1091.648775</td>\n",
       "      <td>-904.862332</td>\n",
       "      <td>-768.730118</td>\n",
       "      <td>-536.614247</td>\n",
       "      <td>-940.066401</td>\n",
       "      <td>-2196.106435</td>\n",
       "      <td>-985.436538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.372415</td>\n",
       "      <td>-54.993100</td>\n",
       "      <td>-21.331203</td>\n",
       "      <td>-42.520671</td>\n",
       "      <td>-69.958454</td>\n",
       "      <td>-36.663614</td>\n",
       "      <td>-6.019517</td>\n",
       "      <td>-58.038648</td>\n",
       "      <td>-96.510577</td>\n",
       "      <td>0.655566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>41.029458</td>\n",
       "      <td>-26.450932</td>\n",
       "      <td>14.150357</td>\n",
       "      <td>-10.002014</td>\n",
       "      <td>-35.709216</td>\n",
       "      <td>-5.305916</td>\n",
       "      <td>22.305288</td>\n",
       "      <td>-18.329733</td>\n",
       "      <td>-57.013161</td>\n",
       "      <td>28.147001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>77.473860</td>\n",
       "      <td>-0.621130</td>\n",
       "      <td>50.025730</td>\n",
       "      <td>21.583519</td>\n",
       "      <td>-8.438641</td>\n",
       "      <td>27.297220</td>\n",
       "      <td>54.594615</td>\n",
       "      <td>18.195365</td>\n",
       "      <td>-24.500331</td>\n",
       "      <td>62.941765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>623.000000</td>\n",
       "      <td>1169.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1487.891279</td>\n",
       "      <td>1156.781213</td>\n",
       "      <td>834.215001</td>\n",
       "      <td>852.355989</td>\n",
       "      <td>569.228259</td>\n",
       "      <td>907.389180</td>\n",
       "      <td>880.135961</td>\n",
       "      <td>868.905853</td>\n",
       "      <td>511.180260</td>\n",
       "      <td>1080.074251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate   q1_frequency   q2_frequency      q1_length   \n",
       "count  404290.000000  404290.000000  404290.000000  404289.000000  \\\n",
       "mean        0.369198       2.827609       3.046961      59.536856   \n",
       "std         0.482588       4.487418       6.026871      29.940546   \n",
       "min         0.000000       1.000000       1.000000       1.000000   \n",
       "25%         0.000000       1.000000       1.000000      39.000000   \n",
       "50%         0.000000       1.000000       1.000000      52.000000   \n",
       "75%         1.000000       3.000000       2.000000      72.000000   \n",
       "max         1.000000      50.000000     120.000000     623.000000   \n",
       "\n",
       "           q2_length  q1_tokens_count  q2_tokens_count  q1_words_count   \n",
       "count  404288.000000    404290.000000    404290.000000   404290.000000  \\\n",
       "mean       60.108663        12.438626        12.697482       10.944592   \n",
       "std        33.863690         6.085369         7.080560        5.431949   \n",
       "min         1.000000         1.000000         1.000000        1.000000   \n",
       "25%        39.000000         9.000000         8.000000        7.000000   \n",
       "50%        51.000000        11.000000        11.000000       10.000000   \n",
       "75%        72.000000        15.000000        15.000000       13.000000   \n",
       "max      1169.000000       144.000000       272.000000      125.000000   \n",
       "\n",
       "       q2_words_count  q1_nonstopwords_count  ...    q2_feat_291   \n",
       "count   404290.000000          404290.000000  ...  404290.000000  \\\n",
       "mean        11.185120               5.646781  ...      46.763162   \n",
       "std          6.311076               3.074383  ...      61.070267   \n",
       "min          1.000000               0.000000  ...   -1181.601844   \n",
       "25%          7.000000               4.000000  ...      10.372415   \n",
       "50%         10.000000               5.000000  ...      41.029458   \n",
       "75%         13.000000               7.000000  ...      77.473860   \n",
       "max        237.000000              57.000000  ...    1487.891279   \n",
       "\n",
       "         q2_feat_292    q2_feat_293    q2_feat_294    q2_feat_295   \n",
       "count  404290.000000  404290.000000  404290.000000  404290.000000  \\\n",
       "mean      -28.009493      12.527336     -11.368422     -43.681345   \n",
       "std        50.926218      68.963110      60.353412      56.719721   \n",
       "min      -869.310289   -1082.525471   -1091.648775    -904.862332   \n",
       "25%       -54.993100     -21.331203     -42.520671     -69.958454   \n",
       "50%       -26.450932      14.150357     -10.002014     -35.709216   \n",
       "75%        -0.621130      50.025730      21.583519      -8.438641   \n",
       "max      1156.781213     834.215001     852.355989     569.228259   \n",
       "\n",
       "         q2_feat_296    q2_feat_297    q2_feat_298    q2_feat_299   \n",
       "count  404290.000000  404290.000000  404290.000000  404290.000000  \\\n",
       "mean       -3.424754      26.239380     -20.802047     -66.431884   \n",
       "std        59.543628      57.218495      68.938349      66.916404   \n",
       "min      -768.730118    -536.614247    -940.066401   -2196.106435   \n",
       "25%       -36.663614      -6.019517     -58.038648     -96.510577   \n",
       "50%        -5.305916      22.305288     -18.329733     -57.013161   \n",
       "75%        27.297220      54.594615      18.195365     -24.500331   \n",
       "max       907.389180     880.135961     868.905853     511.180260   \n",
       "\n",
       "         q2_feat_300  \n",
       "count  404290.000000  \n",
       "mean       36.273753  \n",
       "std        58.818664  \n",
       "min      -985.436538  \n",
       "25%         0.655566  \n",
       "50%        28.147001  \n",
       "75%        62.941765  \n",
       "max      1080.074251  \n",
       "\n",
       "[8 rows x 628 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split data into train test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (404290, 627)\n",
      "Shape of y: (404290,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into X & y first\n",
    "X = data.drop('is_duplicate', axis=1)\n",
    "y = data['is_duplicate']\n",
    "\n",
    "print(\"Shape of X: {0}\".format(X.shape))\n",
    "print(\"Shape of y: {0}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (283003, 627)\n",
      "Shape of X_test: (121287, 627)\n",
      "Shape of y_train: (283003,)\n",
      "Shape of y_test: (121287,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train & test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y)\n",
    "print(\"Shape of X_train: {0}\".format(X_train.shape))\n",
    "print(\"Shape of X_test: {0}\".format(X_test.shape))\n",
    "print(\"Shape of y_train: {0}\".format(y_train.shape))\n",
    "print(\"Shape of y_test: {0}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of target variable in train\n",
      "Class 0: 63.08025003268517 % \n",
      "Class 1: 36.919749967314836 %\n",
      "\n",
      "Distribution of target variable in test\n",
      "Class 0: 63.08013224830361 % \n",
      "Class 1: 36.91986775169639 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of target variable in train\")\n",
    "train_counter = Counter(y_train)\n",
    "train_len = len(y_train)\n",
    "print(\"Class 0: {0} % \\nClass 1: {1} %\".format((train_counter[0]/train_len)*100, (train_counter[1]/train_len)*100))\n",
    "\n",
    "\n",
    "print(\"\\nDistribution of target variable in test\")\n",
    "test_counter = Counter(y_test)\n",
    "test_len = len(y_test)\n",
    "print(\"Class 0: {0} % \\nClass 1: {1} %\".format((test_counter[0]/test_len)*100, (test_counter[1]/test_len)*100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build random model\n",
    "Here we will randomly assign a class based on random probability to each test data point and measure its log loss.<br>\n",
    "A strategy we will follow for this is:\n",
    "1. Generatea list of 2 random numbers for each test row\n",
    "2. Divide each random number by its sum so we get their sum as 1\n",
    "3. Take the index of maximum of the 2 numbers in the list\n",
    "4. This index will be the class of given test row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test log-loss of random model: 0.8883442618472261\n",
      "\n",
      "Test accuracy score of random model: 0.4978522018023366\n",
      "\n",
      "Test confusion matrix of random model: \n",
      "[[38017 38491]\n",
      " [22413 22366]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = np.zeros((test_len,2))\n",
    "for i in range(test_len):\n",
    "    random_probs = np.random.rand(1,2)\n",
    "    y_pred_prob[i] = ((random_probs/sum(sum(random_probs)))[0])\n",
    "\n",
    "print(\"Test log-loss of random model: {0}\".format(log_loss(y_test, y_pred_prob, eps=1e-15)))\n",
    "\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "print(\"\\nTest accuracy score of random model: {0}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print(\"\\nTest confusion matrix of random model: \\n{0}\".format(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil ltake this as benchmark to compare our future models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
