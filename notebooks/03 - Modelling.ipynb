{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pair Similarity\n",
    "### Kaggle Competition link: https://www.kaggle.com/c/quora-question-pairs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have built features to train the model on. Here we will load data with all our 627 features. We will first build a random or simple (Naive Bayes) base model and then will try out different machine learning algorithms and compare against our base model. After that, we will choose the best one and tune it to generalize it on future data.</p> \n",
    "<p> The metrics we will evaluate the models on are:<br>\n",
    "* log-loss <br>\n",
    "* Binary Confusion Matrix <br> \n",
    "</p>\n",
    "\n",
    "Our strategy is:\n",
    "1. Load the data\n",
    "2. Split data into train test (70:30)\n",
    "3. Normalize data\n",
    "4. <b>Build random model:</b> A model that randomly assigns probabilities.\n",
    "5. Apply models with default parameters:<br>\n",
    "   i. <b>Build Logistic Regression:</b> A statistical model that uses a logistic function to model the probability of a binary response based on one or more predictor variables.<br>\n",
    "   ii. <b>Build Naive Bayes:</b> A probabilistic algorithm based on Bayes' theorem that assumes the independence of the features in the input data<br>\n",
    "   iii. <b>Build Support Vector Machines:</b> Works by finding the best hyperplane that separates different classes of data points<br>\n",
    "   iv. <b>Build Gradient Boosting:</b> A powerful ensemble method that combines multiple weak models to create a strong classifier<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# General\n",
    "from datetime import datetime \n",
    "import pickle\n",
    "\n",
    "# Data \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load data from SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "Time taken: 0:02:57.968335\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "try:\n",
    "    conn = sqlite3.connect(\"train.db\")\n",
    "    data = pd.read_sql_query(\"SELECT * FROM train_data ORDER BY RANDOM() LIMIT 100000\", conn)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Data loaded!\\nTime taken: {0}\".format(datetime.now()-start))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (100000, 634)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: {0}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data after removing unnecessary columns: (100000, 628)\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "data = data.iloc[:,6:]\n",
    "print(\"Shape of data after removing unnecessary columns: {0}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_frequency</th>\n",
       "      <th>q2_frequency</th>\n",
       "      <th>q1_length</th>\n",
       "      <th>q2_length</th>\n",
       "      <th>q1_tokens_count</th>\n",
       "      <th>q2_tokens_count</th>\n",
       "      <th>q1_words_count</th>\n",
       "      <th>q2_words_count</th>\n",
       "      <th>q1_nonstopwords_count</th>\n",
       "      <th>...</th>\n",
       "      <th>q2_feat_291</th>\n",
       "      <th>q2_feat_292</th>\n",
       "      <th>q2_feat_293</th>\n",
       "      <th>q2_feat_294</th>\n",
       "      <th>q2_feat_295</th>\n",
       "      <th>q2_feat_296</th>\n",
       "      <th>q2_feat_297</th>\n",
       "      <th>q2_feat_298</th>\n",
       "      <th>q2_feat_299</th>\n",
       "      <th>q2_feat_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.369100</td>\n",
       "      <td>2.823570</td>\n",
       "      <td>3.043510</td>\n",
       "      <td>59.454840</td>\n",
       "      <td>60.111910</td>\n",
       "      <td>12.424970</td>\n",
       "      <td>12.696360</td>\n",
       "      <td>10.932300</td>\n",
       "      <td>11.183960</td>\n",
       "      <td>5.641480</td>\n",
       "      <td>...</td>\n",
       "      <td>46.569629</td>\n",
       "      <td>-27.794024</td>\n",
       "      <td>12.598192</td>\n",
       "      <td>-11.504296</td>\n",
       "      <td>-43.654854</td>\n",
       "      <td>-3.496213</td>\n",
       "      <td>26.121807</td>\n",
       "      <td>-20.646194</td>\n",
       "      <td>-66.488375</td>\n",
       "      <td>36.155795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.482563</td>\n",
       "      <td>4.468338</td>\n",
       "      <td>6.080107</td>\n",
       "      <td>29.794226</td>\n",
       "      <td>33.917819</td>\n",
       "      <td>6.056198</td>\n",
       "      <td>7.102448</td>\n",
       "      <td>5.406961</td>\n",
       "      <td>6.331815</td>\n",
       "      <td>3.059856</td>\n",
       "      <td>...</td>\n",
       "      <td>60.911657</td>\n",
       "      <td>51.105032</td>\n",
       "      <td>68.906294</td>\n",
       "      <td>60.590214</td>\n",
       "      <td>56.855171</td>\n",
       "      <td>59.871539</td>\n",
       "      <td>57.103065</td>\n",
       "      <td>69.285594</td>\n",
       "      <td>67.439770</td>\n",
       "      <td>58.390357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-597.424782</td>\n",
       "      <td>-711.233365</td>\n",
       "      <td>-721.021765</td>\n",
       "      <td>-998.057760</td>\n",
       "      <td>-688.422742</td>\n",
       "      <td>-768.730118</td>\n",
       "      <td>-369.922146</td>\n",
       "      <td>-878.340893</td>\n",
       "      <td>-2196.106435</td>\n",
       "      <td>-364.145416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.068010</td>\n",
       "      <td>-54.789893</td>\n",
       "      <td>-21.192799</td>\n",
       "      <td>-42.669529</td>\n",
       "      <td>-69.730615</td>\n",
       "      <td>-36.950131</td>\n",
       "      <td>-6.184240</td>\n",
       "      <td>-58.039417</td>\n",
       "      <td>-96.491496</td>\n",
       "      <td>0.591541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>40.863403</td>\n",
       "      <td>-26.268981</td>\n",
       "      <td>14.107473</td>\n",
       "      <td>-10.018565</td>\n",
       "      <td>-35.657638</td>\n",
       "      <td>-5.231376</td>\n",
       "      <td>22.242627</td>\n",
       "      <td>-18.285610</td>\n",
       "      <td>-56.951148</td>\n",
       "      <td>28.049228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>77.466833</td>\n",
       "      <td>-0.475320</td>\n",
       "      <td>49.983556</td>\n",
       "      <td>21.570394</td>\n",
       "      <td>-8.413399</td>\n",
       "      <td>27.297220</td>\n",
       "      <td>54.648308</td>\n",
       "      <td>18.541631</td>\n",
       "      <td>-24.297612</td>\n",
       "      <td>62.815105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>1151.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>272.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>914.916410</td>\n",
       "      <td>1156.781213</td>\n",
       "      <td>834.215001</td>\n",
       "      <td>852.355989</td>\n",
       "      <td>569.228259</td>\n",
       "      <td>740.797131</td>\n",
       "      <td>880.135961</td>\n",
       "      <td>868.905853</td>\n",
       "      <td>511.180260</td>\n",
       "      <td>1080.074251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 628 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate   q1_frequency   q2_frequency      q1_length   \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \\\n",
       "mean        0.369100       2.823570       3.043510      59.454840   \n",
       "std         0.482563       4.468338       6.080107      29.794226   \n",
       "min         0.000000       1.000000       1.000000       1.000000   \n",
       "25%         0.000000       1.000000       1.000000      39.000000   \n",
       "50%         0.000000       1.000000       1.000000      52.000000   \n",
       "75%         1.000000       3.000000       2.000000      72.000000   \n",
       "max         1.000000      50.000000     120.000000     370.000000   \n",
       "\n",
       "           q2_length  q1_tokens_count  q2_tokens_count  q1_words_count   \n",
       "count  100000.000000    100000.000000    100000.000000   100000.000000  \\\n",
       "mean       60.111910        12.424970        12.696360       10.932300   \n",
       "std        33.917819         6.056198         7.102448        5.406961   \n",
       "min         2.000000         1.000000         1.000000        1.000000   \n",
       "25%        39.000000         9.000000         8.000000        7.000000   \n",
       "50%        51.000000        11.000000        11.000000       10.000000   \n",
       "75%        71.000000        15.000000        15.000000       13.000000   \n",
       "max      1151.000000       100.000000       272.000000       71.000000   \n",
       "\n",
       "       q2_words_count  q1_nonstopwords_count  ...    q2_feat_291   \n",
       "count   100000.000000          100000.000000  ...  100000.000000  \\\n",
       "mean        11.183960               5.641480  ...      46.569629   \n",
       "std          6.331815               3.059856  ...      60.911657   \n",
       "min          1.000000               0.000000  ...    -597.424782   \n",
       "25%          7.000000               4.000000  ...      10.068010   \n",
       "50%         10.000000               5.000000  ...      40.863403   \n",
       "75%         13.000000               7.000000  ...      77.466833   \n",
       "max        237.000000              53.000000  ...     914.916410   \n",
       "\n",
       "         q2_feat_292    q2_feat_293    q2_feat_294    q2_feat_295   \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \\\n",
       "mean      -27.794024      12.598192     -11.504296     -43.654854   \n",
       "std        51.105032      68.906294      60.590214      56.855171   \n",
       "min      -711.233365    -721.021765    -998.057760    -688.422742   \n",
       "25%       -54.789893     -21.192799     -42.669529     -69.730615   \n",
       "50%       -26.268981      14.107473     -10.018565     -35.657638   \n",
       "75%        -0.475320      49.983556      21.570394      -8.413399   \n",
       "max      1156.781213     834.215001     852.355989     569.228259   \n",
       "\n",
       "         q2_feat_296    q2_feat_297    q2_feat_298    q2_feat_299   \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \\\n",
       "mean       -3.496213      26.121807     -20.646194     -66.488375   \n",
       "std        59.871539      57.103065      69.285594      67.439770   \n",
       "min      -768.730118    -369.922146    -878.340893   -2196.106435   \n",
       "25%       -36.950131      -6.184240     -58.039417     -96.491496   \n",
       "50%        -5.231376      22.242627     -18.285610     -56.951148   \n",
       "75%        27.297220      54.648308      18.541631     -24.297612   \n",
       "max       740.797131     880.135961     868.905853     511.180260   \n",
       "\n",
       "         q2_feat_300  \n",
       "count  100000.000000  \n",
       "mean       36.155795  \n",
       "std        58.390357  \n",
       "min      -364.145416  \n",
       "25%         0.591541  \n",
       "50%        28.049228  \n",
       "75%        62.815105  \n",
       "max      1080.074251  \n",
       "\n",
       "[8 rows x 628 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split data into train test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100000, 627)\n",
      "Shape of y: (100000,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into X & y first\n",
    "X = data.drop('is_duplicate', axis=1)\n",
    "y = data['is_duplicate']\n",
    "\n",
    "print(\"Shape of X: {0}\".format(X.shape))\n",
    "print(\"Shape of y: {0}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (70000, 627)\n",
      "Shape of X_test: (30000, 627)\n",
      "Shape of y_train: (70000,)\n",
      "Shape of y_test: (30000,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train & test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y)\n",
    "print(\"Shape of X_train: {0}\".format(X_train.shape))\n",
    "print(\"Shape of X_test: {0}\".format(X_test.shape))\n",
    "print(\"Shape of y_train: {0}\".format(y_train.shape))\n",
    "print(\"Shape of y_test: {0}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of target variable in train\n",
      "Class 0: 63.09 % \n",
      "Class 1: 36.91 %\n",
      "\n",
      "Distribution of target variable in test\n",
      "Class 0: 63.09 % \n",
      "Class 1: 36.91 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of target variable in train\")\n",
    "train_counter = Counter(y_train)\n",
    "train_len = len(y_train)\n",
    "print(\"Class 0: {0} % \\nClass 1: {1} %\".format((train_counter[0]/train_len)*100, (train_counter[1]/train_len)*100))\n",
    "\n",
    "\n",
    "print(\"\\nDistribution of target variable in test\")\n",
    "test_counter = Counter(y_test)\n",
    "test_len = len(y_test)\n",
    "print(\"Class 0: {0} % \\nClass 1: {1} %\".format((test_counter[0]/test_len)*100, (test_counter[1]/test_len)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 0\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Normalize data\n",
    "Before we proceed to build the models, lets normalize all the features first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X_train\n",
      "Normalization of X_train is completed.\n",
      "\n",
      "Time taken: 0:00:04.362978\n",
      "\n",
      "Normalizing X_test\n",
      "Normalization of X_test is completed.\n",
      "\n",
      "Time taken: 0:00:00.401804\n"
     ]
    }
   ],
   "source": [
    "numerical_pipeline = Pipeline(steps=[(\"normalizer\", MinMaxScaler()), ((\"imputer\", SimpleImputer(strategy=\"most_frequent\")))])\n",
    "\n",
    "vectorizer = ColumnTransformer([(\"num_pipeline\", numerical_pipeline, numerical_features)])\n",
    "\n",
    "start = datetime.now()\n",
    "print(\"Vectorizing X_train\")\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "print(\"Normalization of X_train is completed.\\n\\nTime taken: {0}\".format(datetime.now()-start))\n",
    "\n",
    "start = datetime.now()\n",
    "print(\"\\nNormalizing X_test\")\n",
    "X_test = vectorizer.transform(X_test)\n",
    "print(\"Normalization of X_test is completed.\\n\\nTime taken: {0}\".format(datetime.now()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped the vectorizer in ../models/vectorizer.pkl file\n"
     ]
    }
   ],
   "source": [
    "# Lets save our vectorizer to .pkl file\n",
    "vectorizer_file = \"../models/vectorizer.pkl\"\n",
    "with open(vectorizer_file, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print('Dumped the vectorizer in {} file'.format(vectorizer_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build random model\n",
    "Here we will randomly assign a class based on random probability to each test data point and measure its log loss.<br>\n",
    "A strategy we will follow for this is:\n",
    "1. Generatea list of 2 random numbers for each test row\n",
    "2. Divide each random number by its sum so we get their sum as 1\n",
    "3. Take the index of maximum of the 2 numbers in the list\n",
    "4. This index will be the class of given test row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test log-loss of random model: 0.89532403134099\n",
      "\n",
      "Test accuracy score of random model: 0.4961\n",
      "\n",
      "Test confusion matrix of random model: \n",
      "[[9373 9554]\n",
      " [5563 5510]]\n",
      "\n",
      "Test confusion matrix of random model (%): \n",
      "[[31.24 31.85]\n",
      " [18.54 18.37]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = np.zeros((test_len,2))\n",
    "for i in range(test_len):\n",
    "    random_probs = np.random.rand(1,2)\n",
    "    y_pred_prob[i] = ((random_probs/sum(sum(random_probs)))[0])\n",
    "\n",
    "print(\"Test log-loss of random model: {0}\".format(log_loss(y_test, y_pred_prob, eps=1e-15)))\n",
    "\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "print(\"\\nTest accuracy score of random model: {0}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print(\"\\nTest confusion matrix of random model: \\n{0}\".format(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "print(\"\\nTest confusion matrix of random model (%): \\n{0}\".format(np.round(confusion_matrix(y_test, y_pred)/len(y_test)*100,2)))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil ltake this as benchmark to compare our future models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Apply ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression started.\n",
      "LogisticRegression training completed. Time taken: 0:00:47.715623\n",
      "\n",
      "BernoulliNB started.\n",
      "BernoulliNB training completed. Time taken: 0:00:00.554632\n",
      "\n",
      "SVC started.\n",
      "SVC training completed. Time taken: 0:22:43.016640\n",
      "\n",
      "GradientBoostingClassifier started.\n",
      "GradientBoostingClassifier training completed. Time taken: 0:51:01.605331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for classifier in [LogisticRegression(solver='lbfgs', max_iter=3000), BernoulliNB(), SVC(), GradientBoostingClassifier() ]:\n",
    "    \n",
    "    # Training\n",
    "    start = datetime.now()\n",
    "    clf_str = str(classifier).split(\"(\")[0]\n",
    "    print(\"{0} started.\".format(clf_str))\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"{0} training completed. Time taken: {1}\\n\".format(clf_str, datetime.now()-start))\n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    lg_loss = log_loss(y_test,y_pred)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    cm = np.round((confusion_matrix(y_test,y_pred)/len(X_test)*100),2)\n",
    "    \n",
    "    # Add to result\n",
    "    temp = list()\n",
    "    temp.append(clf_str)\n",
    "    temp.append(\"Default\")\n",
    "    temp.append(lg_loss)\n",
    "    temp.append(acc)\n",
    "    temp.append(cm)\n",
    "    temp.append(datetime.now()-start)\n",
    "    result.append(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Log-loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Confusion Matrix (TP,FP,FN,TN)</th>\n",
       "      <th>Time taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>Default</td>\n",
       "      <td>8.010101</td>\n",
       "      <td>0.777767</td>\n",
       "      <td>[[53.57, 9.52], [12.7, 24.21]]</td>\n",
       "      <td>0 days 00:00:47.783151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>Default</td>\n",
       "      <td>12.445874</td>\n",
       "      <td>0.654700</td>\n",
       "      <td>[[38.9, 24.19], [10.34, 26.57]]</td>\n",
       "      <td>0 days 00:00:00.805230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>Default</td>\n",
       "      <td>7.282019</td>\n",
       "      <td>0.797967</td>\n",
       "      <td>[[53.89, 9.2], [11.0, 25.91]]</td>\n",
       "      <td>0 days 00:34:24.827754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>Default</td>\n",
       "      <td>6.320855</td>\n",
       "      <td>0.824633</td>\n",
       "      <td>[[56.07, 7.02], [10.51, 26.4]]</td>\n",
       "      <td>0 days 00:51:01.899364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Algorithm Hyperparameters   Log-loss  Accuracy   \n",
       "0          LogisticRegression         Default   8.010101  0.777767  \\\n",
       "1                 BernoulliNB         Default  12.445874  0.654700   \n",
       "2                         SVC         Default   7.282019  0.797967   \n",
       "3  GradientBoostingClassifier         Default   6.320855  0.824633   \n",
       "\n",
       "    Confusion Matrix (TP,FP,FN,TN)             Time taken  \n",
       "0   [[53.57, 9.52], [12.7, 24.21]] 0 days 00:00:47.783151  \n",
       "1  [[38.9, 24.19], [10.34, 26.57]] 0 days 00:00:00.805230  \n",
       "2    [[53.89, 9.2], [11.0, 25.91]] 0 days 00:34:24.827754  \n",
       "3   [[56.07, 7.02], [10.51, 26.4]] 0 days 00:51:01.899364  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result, columns=['Algorithm', 'Hyperparameters', 'Log-loss', 'Accuracy', 'Confusion Matrix (TP,FP,FN,TN)', 'Time taken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
